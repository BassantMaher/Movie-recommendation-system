{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Or6LDv1aW7iJ"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomOverSampler\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KNeighborsClassifier\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_val_score\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\imblearn\\__init__.py:52\u001b[0m\n\u001b[0;32m     48\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial import of imblearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     53\u001b[0m         combine,\n\u001b[0;32m     54\u001b[0m         ensemble,\n\u001b[0;32m     55\u001b[0m         exceptions,\n\u001b[0;32m     56\u001b[0m         metrics,\n\u001b[0;32m     57\u001b[0m         over_sampling,\n\u001b[0;32m     58\u001b[0m         pipeline,\n\u001b[0;32m     59\u001b[0m         tensorflow,\n\u001b[0;32m     60\u001b[0m         under_sampling,\n\u001b[0;32m     61\u001b[0m         utils,\n\u001b[0;32m     62\u001b[0m     )\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionSampler\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\imblearn\\combine\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The :mod:`imblearn.combine` provides methods which combine\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mover-sampling and under-sampling.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_enn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTEENN\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_tomek\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTEENN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTETomek\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\imblearn\\combine\\_smote_enn.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseSampler\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseOverSampler\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\imblearn\\base.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_classification_targets\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_sampling_strategy, check_target_type\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArraysTransformer\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSamplerMixin\u001b[39;00m(BaseEstimator, metaclass\u001b[38;5;241m=\u001b[39mABCMeta):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\imblearn\\utils\\_param_validation.py:908\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_valid_param  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m    907\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m--> 908\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    909\u001b[0m     HasMethods,\n\u001b[0;32m    910\u001b[0m     Hidden,\n\u001b[0;32m    911\u001b[0m     Interval,\n\u001b[0;32m    912\u001b[0m     Options,\n\u001b[0;32m    913\u001b[0m     StrOptions,\n\u001b[0;32m    914\u001b[0m     _ArrayLikes,\n\u001b[0;32m    915\u001b[0m     _Booleans,\n\u001b[0;32m    916\u001b[0m     _Callables,\n\u001b[0;32m    917\u001b[0m     _CVObjects,\n\u001b[0;32m    918\u001b[0m     _InstancesOf,\n\u001b[0;32m    919\u001b[0m     _IterablesNotString,\n\u001b[0;32m    920\u001b[0m     _MissingValues,\n\u001b[0;32m    921\u001b[0m     _NoneConstraint,\n\u001b[0;32m    922\u001b[0m     _PandasNAConstraint,\n\u001b[0;32m    923\u001b[0m     _RandomStates,\n\u001b[0;32m    924\u001b[0m     _SparseMatrices,\n\u001b[0;32m    925\u001b[0m     _VerboseHelper,\n\u001b[0;32m    926\u001b[0m     make_constraint,\n\u001b[0;32m    927\u001b[0m     validate_params,\n\u001b[0;32m    928\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import tensorflow.keras as keras\n",
    "from keras.layers import Input, Embedding, Flatten, Dense, Concatenate, Dot, Dropout\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NnXk6HLW_C6"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('netflix_titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "id": "doit7AnZXtHl",
    "outputId": "ebbc1e49-e8af-436b-eeb8-6794a0570cb5"
   },
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hgdv7yMWXC7Q",
    "outputId": "4c0f774e-a1ad-4ae6-c626-d1c46c022b41"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "A5cDBC_nX1SN",
    "outputId": "ec2ad79d-a06b-4c21-b325-b54fb5a49613"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjVbBbZSW5Jn"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8VgdCZ1TqFZ"
   },
   "outputs": [],
   "source": [
    "# convert date_added to pandas datetime type\n",
    "df['date_added'] = pd.to_datetime(df['date_added'],format=\"mixed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RWcqkEd7TqmR",
    "outputId": "48f477af-8b38-4d14-d8a9-4f86db8571ae"
   },
   "outputs": [],
   "source": [
    "# fill null values in director, cast and country columns to be set to 'unknown', and for duration to be 'Not added'\n",
    "df[\"director\"]=df[\"director\"].fillna(\"Unknown\")\n",
    "df[\"cast\"]=df[\"cast\"].fillna(\"Unknown\")\n",
    "df[\"country\"]=df[\"country\"].fillna(\"Unknown\")\n",
    "df['duration'].replace(np.nan,'Not Added', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJlc-NAcXFbn"
   },
   "outputs": [],
   "source": [
    "# fill the rest of null values to previous and next values\n",
    "df = df.ffill().bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YPHOZpnhQagz"
   },
   "outputs": [],
   "source": [
    "# drop rows have 'duration' == 'Not Added'\n",
    "idx = df[df.duration == 'Not Added'].index\n",
    "df.drop(idx, inplace= True)\n",
    "df.reset_index(drop= True, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFluRESPXHdG"
   },
   "outputs": [],
   "source": [
    "# Create features year_added, month_added & month_name\n",
    "df['year_added'] = df['date_added'].dt.year.astype(int)\n",
    "df['month_added'] = df['date_added'].dt.month\n",
    "df['month_name'] = df['date_added'].dt.month_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QiacJBFsXJ60"
   },
   "outputs": [],
   "source": [
    "# create 'season_count' col for TV Shows & 'duration' col for movies\n",
    "df['season_count'] = df.duration.apply(lambda x: x.split(' ')[0] if 'Season' in x else np.nan)\n",
    "df['duration'] = df.duration.apply(lambda x: x.split(' ')[0] if 'Season' not in x else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ParOojQyXKpL"
   },
   "outputs": [],
   "source": [
    "# convert ['duration', 'release_year', 'season_count'] to numeric format\n",
    "cols = ['duration', 'release_year', 'season_count']\n",
    "df[cols] = df[cols].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "70FiTvvCemXq",
    "outputId": "4e8eb672-2e20-4d7a-dc6b-0b080f55b660"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HWH-zkGIeB4K",
    "outputId": "3d45471b-2c0c-4362-9651-9d086bb0063a"
   },
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mT_AbnsjXEV4"
   },
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "eWxkY37zYQpB",
    "outputId": "79d868d9-1e2d-4b1c-f846-b58e49c08d05"
   },
   "outputs": [],
   "source": [
    "type_counts = df['type'].value_counts().reset_index()\n",
    "type_counts.columns = ['type', 'count']\n",
    "\n",
    "fig = px.bar(type_counts, x='count', y='type', orientation='h', color='type',\n",
    "             color_discrete_sequence=px.colors.qualitative.Dark2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "pr6OJrMMMSQS",
    "outputId": "03850cf5-9673-42cb-f06b-1a0d47b3ee1c"
   },
   "outputs": [],
   "source": [
    "# Calculate the top 10 countries with the most content\n",
    "top_countries = df['country'].value_counts().nlargest(10).reset_index()\n",
    "top_countries.columns = ['country', 'count']\n",
    "\n",
    "fig = px.bar(top_countries, x='count', y='country', orientation='h',\n",
    "             title='Top 10 countries with most content',\n",
    "             labels={'count': 'Count', 'country': 'Country'},\n",
    "             color='country',  # Differentiate colors by country\n",
    "             color_discrete_sequence=px.colors.qualitative.Safe)  # Use a qualitative color sequence\n",
    "\n",
    "fig.update_layout(\n",
    "    yaxis=dict(title='Country'),\n",
    "    xaxis=dict(title='Count'),\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gj93TmfYUYCD"
   },
   "outputs": [],
   "source": [
    "# Split the DataFrame into movies and TV shows\n",
    "df_movies = df[df['type'] == 'Movie']\n",
    "df_shows = df[df['type'] == 'TV Show']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18VXij4aY_kB",
    "outputId": "f349631f-48cf-42bc-93df-315da0bfd1e0"
   },
   "outputs": [],
   "source": [
    "# How many Horror Movies and Tv Shows on Netflix?\n",
    "print('no. of Horror movies on Netflix: ',df_movies[\"listed_in\"].str.contains(\"Horror\").sum())\n",
    "print('no. of Horror series on TV shows: ', df_shows[\"listed_in\"].str.contains(\"Horror\").sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Poamp-8JZAZW",
    "outputId": "04d9641e-7935-41c3-c7b4-86302d8f1a1b"
   },
   "outputs": [],
   "source": [
    "# What is the average duration of the movie?\n",
    "df_movies.duration.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "L_jtGZT8ZChp",
    "outputId": "62c53dc0-7109-4097-b212-4c0b71021f7b"
   },
   "outputs": [],
   "source": [
    "# Create a summary statstics table for duration and season count features using dedicated pandas function\n",
    "df_movies.duration.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "tERS-38hZD5b",
    "outputId": "9de7e089-b398-4fbb-f85b-a308b00c6395"
   },
   "outputs": [],
   "source": [
    "df_shows.season_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "jaUu8PQbZFb8",
    "outputId": "78bf92a9-ce0b-4e70-a81f-eaaf7be6e049"
   },
   "outputs": [],
   "source": [
    "# longest movie ever\n",
    "df_movies[df_movies.duration == df_movies.duration.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "kcEf0w2HZG3T",
    "outputId": "ee65d1b2-9639-46b8-98f2-74e16352bf79"
   },
   "outputs": [],
   "source": [
    "# top 10 countries in Movie releases\n",
    "df_grouped = df_movies.groupby(\"country\")[[\"show_id\"]].count().sort_values(by=\"show_id\", ascending=False).head(10).reset_index()\n",
    "df_grouped = df_grouped.rename(columns= {'show_id': 'count'})\n",
    "fig = px.bar(df_grouped, x= 'country', y= 'count', color= 'count', color_continuous_scale= 'purp')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "vj9zd9OyZH-l",
    "outputId": "c9329aca-7a32-4570-9cd3-32c054b5dd34"
   },
   "outputs": [],
   "source": [
    "# top 10 countries in Tv-Show releases\n",
    "df_grouped = df_shows.groupby(\"country\")[[\"show_id\"]].count().sort_values(by=\"show_id\", ascending=False).head(10).reset_index()\n",
    "df_grouped = df_grouped.rename(columns= {'show_id': 'count'})\n",
    "fig = px.bar(df_grouped, x= 'country', y= 'count', color= 'count', color_continuous_scale= 'inferno')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "V_nCjR44ZM-L",
    "outputId": "038e5f14-ef97-43c6-f7c0-f2b60f86aecf"
   },
   "outputs": [],
   "source": [
    "# What are movies for Tom Cruise\n",
    "df_movies[df_movies[\"cast\"].str.contains(\"Tom Cruise\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "i2URxdJkZPI7",
    "outputId": "f2624633-7bfd-4137-a2a5-04e142e8b7f3"
   },
   "outputs": [],
   "source": [
    "# create line chart showing for each release_year the nu of movies or series added to netflix\n",
    "df_grouped_movies = df_movies.groupby('release_year')['show_id'].count().reset_index().sort_values('release_year')\n",
    "df_grouped_movies.rename(columns= {'show_id': 'count'}, inplace= True)\n",
    "df_grouped_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IXcMU5VxZRg_",
    "outputId": "b5cbb2b2-889e-4756-b952-565c9c94e9f5"
   },
   "outputs": [],
   "source": [
    "df_grouped_series = df_shows.groupby('release_year')['show_id'].count().reset_index().sort_values('release_year')\n",
    "df_grouped_series.rename(columns= {'show_id': 'count'}, inplace= True)\n",
    "df_grouped_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "PC530DbvZU16",
    "outputId": "232a4a45-7237-41d8-fd9f-e1e0c54382c1"
   },
   "outputs": [],
   "source": [
    "# what's the longest movie\n",
    "longest_movie = df_movies.loc[df_movies.duration.idxmax(), 'title']\n",
    "longest_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "aBHJfO4kZWO0",
    "outputId": "b55f0016-2670-496d-8594-e24f579a1f63"
   },
   "outputs": [],
   "source": [
    "# show histogram for movies duration with annotation indicates the longest movie\n",
    "fig = px.histogram(df_movies, x= 'duration')\n",
    "fig.update_layout(annotations= [{'showarrow': True, 'arrowhead': 3, 'x': 312, 'y': 1, 'text': longest_movie}])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "id": "emc62diZZXf6",
    "outputId": "fa248b1f-e4a9-4a63-c87a-cb126875a30e"
   },
   "outputs": [],
   "source": [
    "# show below table\n",
    "df_grouped = df.type.value_counts().reset_index()\n",
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "KkZ5-9lcZYor",
    "outputId": "1499d26f-1570-4009-f109-3ac9c5593474"
   },
   "outputs": [],
   "source": [
    "# show pie chart for df.type\n",
    "fig = px.pie(df_grouped, values= 'count', names= 'type', hole= 0.3)\n",
    "fig.update_traces(textinfo='percent+label')\n",
    "fig.update_layout({'title': {'text': 'Movies & TV Shows Percentages', 'x': 0.5, 'y': 0.95}})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "DCkXX0EeZZ-5",
    "outputId": "9e2177a8-ab63-4034-adf0-78c3f8097c42"
   },
   "outputs": [],
   "source": [
    "# Genre Distribution\n",
    "genre_counts = df['listed_in'].str.split(', ').explode().value_counts().reset_index()\n",
    "genre_counts.columns = ['Genre', 'Count']\n",
    "fig = px.bar(genre_counts, x='Genre', y='Count', title='Genre Distribution on Netflix')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('preprocessed_netflix_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1hV2emSbDWn"
   },
   "source": [
    "# Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OEggzpNGPOjf",
    "outputId": "a5f52e5f-0b91-46af-9110-89df07653ae3"
   },
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X = df.drop(columns=['type'])  # Features\n",
    "y = df['type']  # Target variable\n",
    "\n",
    "# Apply oversampling to address class imbalance\n",
    "oversampler = RandomOverSampler()\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
    "\n",
    "# Convert back to DataFrame if needed\n",
    "df_resampled = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.DataFrame(y_resampled, columns=['type'])], axis=1)\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "X_resampled_encoded = X_resampled.apply(label_encoder.fit_transform)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled_encoded, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initializing classifiers\n",
    "rf_classifier = RandomForestClassifier()\n",
    "logistic_classifier = LogisticRegression()\n",
    "\n",
    "# Fit the models on the training set\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "logistic_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on both the training and test sets for both classifiers\n",
    "# Random Forest\n",
    "rf_train_pred = rf_classifier.predict(X_train)\n",
    "rf_test_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Logistic Regression\n",
    "logistic_train_pred = logistic_classifier.predict(X_train)\n",
    "logistic_test_pred = logistic_classifier.predict(X_test)\n",
    "\n",
    "# Evaluating RandomForestClassifier (train and test accuracy)\n",
    "rf_train_accuracy = accuracy_score(y_train, rf_train_pred)\n",
    "rf_test_accuracy = accuracy_score(y_test, rf_test_pred)\n",
    "\n",
    "# Evaluating Logistic Regression (train and test accuracy)\n",
    "logistic_train_accuracy = accuracy_score(y_train, logistic_train_pred)\n",
    "logistic_test_accuracy = accuracy_score(y_test, logistic_test_pred)\n",
    "\n",
    "# Print the train and test accuracies\n",
    "print(\"RandomForestClassifier Train Accuracy:\", rf_train_accuracy)\n",
    "print(\"RandomForestClassifier Test Accuracy:\", rf_test_accuracy)\n",
    "\n",
    "print(\"Logistic Regression Train Accuracy:\", logistic_train_accuracy)\n",
    "print(\"Logistic Regression Test Accuracy:\", logistic_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rNXNwnUXMuv"
   },
   "source": [
    "# Content-Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvxaEXzDlWjW"
   },
   "outputs": [],
   "source": [
    "# Create a TF-IDF Vectorizer for the 'description' column\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "df['description'] = df['description'].fillna('')\n",
    "tfidf_matrix = tfidf.fit_transform(df['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5D5071sleIO"
   },
   "outputs": [],
   "source": [
    "# Compute cosine similarity\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "pKM6z28Xlgpe",
    "outputId": "170b95d5-aa42-4565-acc3-a646cf9447bc"
   },
   "outputs": [],
   "source": [
    "# Function to get recommendations based on cosine similarity\n",
    "def get_recommendations(title, cosine_sim=cosine_sim):\n",
    "    idx = df[df['title'] == title].index[0]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    return df['title'].iloc[movie_indices]\n",
    "\n",
    "# Test the recommendation system\n",
    "print(get_recommendations('Avengers: Infinity War'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwAQ_YBuXzOK"
   },
   "source": [
    "# KNN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m0MpLUZAPS0s",
    "outputId": "70b1c710-4fa0-4536-85e3-b338794b7488"
   },
   "outputs": [],
   "source": [
    "# Initialize K-Nearest Neighbors classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the KNN model\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with KNN\n",
    "y_train_pred = knn_classifier.predict(X_train)\n",
    "y_test_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the training and test accuracies\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the train and test accuracies\n",
    "print(f\"K-Nearest Neighbors Classifier Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"K-Nearest Neighbors Classifier Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KP4lkJ5K5rua",
    "outputId": "9bfab074-70e2-43c2-beea-47f05385c58d"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train) # Create and assign value to X_train_scaled\n",
    "\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=10)\n",
    "scores = cross_val_score(knn_classifier, X_train_scaled, y_train, cv=5)\n",
    "print(f\"Cross-Validation Accuracy: {scores.mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Assuming your KNN model is stored in the variable 'knn_classifier'\n",
    "model_path = 'knn_model.pkl'  # Specify the path where you want to save the model\n",
    "joblib.dump(knn_classifier, model_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dk5ei43YBlg"
   },
   "source": [
    "# Combined Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GV_U1cwbSxSm",
    "outputId": "6f6b5bcf-24fb-46e9-fdfb-f989046f196b"
   },
   "outputs": [],
   "source": [
    "# Step 1: Preprocess the data\n",
    "# Fill missing values with an empty string\n",
    "df['director'] = df['director'].fillna('')\n",
    "df['cast'] = df['cast'].fillna('')\n",
    "df['listed_in'] = df['listed_in'].fillna('')\n",
    "df['description'] = df['description'].fillna('')\n",
    "df['duration'] = df['duration'].fillna('')\n",
    "\n",
    "# Handle the duration column\n",
    "def process_duration(duration):\n",
    "  duration = str(duration)\n",
    "  if 'min' in duration:\n",
    "         return int(duration.replace(' min', ''))  # Extract minutes\n",
    "  elif 'Season' in duration:\n",
    "         return int(duration.replace(' Seasons', '').replace(' Season', '')) * 60  # Convert seasons to hours (as a proxy)\n",
    "  else:\n",
    "         return 0\n",
    "\n",
    "df['duration'] = df['duration'].fillna('0').apply(process_duration)\n",
    "\n",
    "# Step 2: Combine features including description for similarity computation\n",
    "df['combined_features'] = df['type'] + ' ' + df['director'] + ' ' + df['cast'] + ' ' + df['listed_in'] + ' ' + df['description']\n",
    "\n",
    "# Step 3: Vectorize the text data using TF-IDF\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df['combined_features'])\n",
    "\n",
    "# Step 4: Compute cosine similarity between all movies/shows\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Step 5: Build a function that recommends shows or movies based on similarity score\n",
    "def get_recommendations(title, cosine_sim=cosine_sim):\n",
    "    # Try to get the index of the movie that matches the title\n",
    "    try:\n",
    "        idx = df[df['title'].str.contains(title, case=False)].index[0]\n",
    "    except IndexError:\n",
    "        return \"Sorry, the title you entered was not found in the dataset.\"\n",
    "\n",
    "    # Get the pairwise similarity scores of all shows/movies with that title\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the shows/movies based on similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar shows/movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # Get the show/movie indices\n",
    "    show_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar shows/movies\n",
    "    return df['title'].iloc[show_indices]\n",
    "\n",
    "# Step 6: Test the recommendation function with error handling\n",
    "print(get_recommendations('Avengers: Infinity War'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SQpEMIhbVieP",
    "outputId": "ce19ec28-d87c-464e-f0ad-3a6e170552d6"
   },
   "outputs": [],
   "source": [
    "# Combining text features for content-based similarity\n",
    "df['combined_features'] = df['type'] + ' ' + df['director'] + ' ' + df['cast'] + ' ' + df['listed_in'] + ' ' + df['description']\n",
    "\n",
    "# Step 1: Content-Based Model (TF-IDF + Cosine Similarity)\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df['combined_features'])\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# KNN Preparation\n",
    "# Step 2: KNN Model (for classification)\n",
    "\n",
    "# Separate features and target variable for KNN\n",
    "X = df.drop(columns=['type'])  # Features\n",
    "y = df['type']  # Target variable\n",
    "\n",
    "# Apply oversampling to address class imbalance\n",
    "oversampler = RandomOverSampler()\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
    "\n",
    "# Encode categorical variables for KNN\n",
    "label_encoder = LabelEncoder()\n",
    "X_resampled_encoded = X_resampled.apply(label_encoder.fit_transform)\n",
    "\n",
    "# Feature scaling (KNN benefits from feature scaling)\n",
    "scaler = StandardScaler()\n",
    "X_resampled_scaled = scaler.fit_transform(X_resampled_encoded)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled_scaled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize K-Nearest Neighbors classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the KNN model\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Function to get recommendations from content-based model\n",
    "def get_content_recommendations(title, cosine_sim=cosine_sim):\n",
    "    try:\n",
    "        idx = df[df['title'].str.contains(title, case=False)].index[0]\n",
    "    except IndexError:\n",
    "        return []\n",
    "\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    show_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    return df['title'].iloc[show_indices]\n",
    "\n",
    "# Voting Classifier: Combine KNN and Content-Based Model\n",
    "def voting_classifier(title, cosine_sim=cosine_sim, knn_model=knn_classifier, weight_knn=0.5, weight_content=0.5):\n",
    "    # Step 1: Get content-based recommendations\n",
    "    content_based_recommendations = get_content_recommendations(title)\n",
    "\n",
    "    # Step 2: Get KNN predictions\n",
    "    # Note: We use KNN to predict the \"type\" (TV Show/Movie) based on numeric and categorical features\n",
    "    # For simplicity, we simulate a scenario where we want to predict the type of the given title.\n",
    "    try:\n",
    "        idx = df[df['title'].str.contains(title, case=False)].index[0]\n",
    "        knn_pred = knn_model.predict([X_resampled_scaled[idx]])[0]  # KNN predicts the type\n",
    "    except IndexError:\n",
    "        return \"Title not found in the dataset.\"\n",
    "\n",
    "    # Step 3: Combine content-based and KNN results\n",
    "    if len(content_based_recommendations) > 0:\n",
    "        return {\n",
    "            \"KNN_Prediction\": knn_pred,\n",
    "            \"Content-Based_Recommendations\": content_based_recommendations,\n",
    "        }\n",
    "    else:\n",
    "        return \"No content-based recommendations found.\"\n",
    "\n",
    "# Test the combined voting classifier with a specific title\n",
    "result = voting_classifier('Avengers: Infinity War')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tnHM3CmuVv-e",
    "outputId": "cf8ccafe-24c3-4a3c-9887-14f5fdcbec64"
   },
   "outputs": [],
   "source": [
    "# Test the recommendation system with another movie or TV show\n",
    "result = voting_classifier('Dumb And Dumber')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r7ptwT-Rdhyt",
    "outputId": "3254271b-d396-40bf-c93c-2b4b1ad91e62"
   },
   "outputs": [],
   "source": [
    "# Test the recommendation system with another movie or TV show\n",
    "result = voting_classifier('Black Mirror')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxJ4_2CQAV3m",
    "outputId": "406136f6-05de-4bd5-87fe-0075b9791ff1"
   },
   "outputs": [],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "id": "8tAd-ZC1ANaM",
    "outputId": "b989d062-8ef5-47c2-a393-9279eefcf55d"
   },
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WYDiVNmSLWV"
   },
   "source": [
    "# Deep learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgZcguMMSNzA"
   },
   "source": [
    "TF-IDF uses word counts and document frequency to measure the importance of words, whereas word embeddings like GloVe use pre-trained vectors that capture semantic meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaNJ80uxSSBk"
   },
   "source": [
    "1.   Load GloVe embeddings: Download the GloVe embeddings file and load it into a dictionary.\n",
    "2.   Preprocess your text data: Tokenize the descriptions and map each word to its corresponding GloVe vector.\n",
    "3.   Generate document embeddings: Instead of calculating TF-IDF, compute the document embedding as the average (or weighted average) of the GloVe embeddings for the words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J6Aj4FhPSKGy",
    "outputId": "7c1d75c7-3d6d-495c-fb8b-ac071419f749"
   },
   "outputs": [],
   "source": [
    "!curl --header 'Host: storage.googleapis.com' --user-agent 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:130.0) Gecko/20100101 Firefox/130.0' --header 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/png,image/svg+xml,*/*;q=0.8' --header 'Accept-Language: en-US,en;q=0.5' --header 'Upgrade-Insecure-Requests: 1' --header 'Sec-Fetch-Dest: document' --header 'Sec-Fetch-Mode: navigate' --header 'Sec-Fetch-Site: none' --header 'Sec-Fetch-User: ?1' 'https://storage.googleapis.com/kaggle-data-sets/715814/1246668/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240928%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240928T221902Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=0ef866cc29837053951c53711e12ac68b6125a13fb09c9ff71b39c8e30de68481d27580976dea537673649f2072cee1621f5fe5dd1c26df71e74e78a925f26f8173a09052a97780f026be2ee3678abdba5779eb14622733e8360c70dd2c0274f50910bf99c72e41195ad74311c78c5f3aacbeca31b0d19f1181c231e2fa15e1b604245c2ff39aa50888691e3d8232f2cdc709a745a1a5b092eae6d157c192bcf5064fa693cdec9ebec3ef56e2000ce455e725a04654c877fe68bc049da1aa0c742dac4aceddee7bde6cbd643e91f9e6943fb1b8842694cc7380cd7b6e8abc3d61842c9c0f4b8b055e4865b777e53616e00351ad4024e5d57f0823590a91542e3' --output 'archive.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GpmEbr6wG1Me",
    "outputId": "e368e14e-bc9f-40c0-c079-22be069fd14a"
   },
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove*.zip -d glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OdYBn06ESwJJ",
    "outputId": "41fdd71d-3c73-4749-c7bc-6ff877b159c8"
   },
   "outputs": [],
   "source": [
    "!unzip archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wf_IJOMeS1vt",
    "outputId": "162c7f21-018f-4061-c0eb-6391e87721d6"
   },
   "outputs": [],
   "source": [
    "# 1. Load GloVe embeddings\n",
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            embedding = np.array(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = embedding\n",
    "    return embeddings_index\n",
    "\n",
    "# Path to your GloVe file\n",
    "glove_file = 'glove/glove.6B.100d.txt'\n",
    "glove_embeddings = load_glove_embeddings(glove_file)\n",
    "\n",
    "# 2. Tokenize and preprocess text\n",
    "def preprocess(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# 3. Get GloVe embedding for each word in the text\n",
    "def get_document_embedding(text, glove_embeddings, embedding_dim=100):\n",
    "    tokens = preprocess(text)\n",
    "    valid_embeddings = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in glove_embeddings:\n",
    "            valid_embeddings.append(glove_embeddings[token])\n",
    "\n",
    "    if valid_embeddings:\n",
    "        # Average the embeddings (you can also use weighted averages)\n",
    "        doc_embedding = np.mean(valid_embeddings, axis=0)\n",
    "    else:\n",
    "        # If no valid embeddings are found, return a zero vector\n",
    "        doc_embedding = np.zeros(embedding_dim)\n",
    "\n",
    "    return doc_embedding\n",
    "\n",
    "# Apply this to your dataframe\n",
    "df['description'] = df['description'].fillna('')\n",
    "embedding_dim = 100  # Adjust according to the GloVe version you're using\n",
    "\n",
    "# Create document embeddings for each description\n",
    "embeddings = np.array([get_document_embedding(desc, glove_embeddings, embedding_dim) for desc in tqdm(df['description'])])\n",
    "\n",
    "# embeddings now contains GloVe-based vector representations for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NArlbJFfS9Ds",
    "outputId": "c75a5862-b426-4086-d389-ee113f9a9ed8"
   },
   "outputs": [],
   "source": [
    "# 1. Function to compute cosine similarity between document embeddings\n",
    "def compute_cosine_similarity(embeddings):\n",
    "    return cosine_similarity(embeddings)\n",
    "\n",
    "# Compute the cosine similarity matrix using GloVe embeddings\n",
    "cosine_sim_glove = compute_cosine_similarity(embeddings)\n",
    "\n",
    "# 2. Function to get recommendations based on GloVe cosine similarity\n",
    "def get_glove_recommendations(title, cosine_sim=cosine_sim_glove):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = df[df['title'] == title].index[0]\n",
    "\n",
    "    # Get the pairwise similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]  # Skipping the first movie (itself)\n",
    "\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar movies\n",
    "    return df['title'].iloc[movie_indices]\n",
    "\n",
    "# Test the recommendation system with GloVe embeddings\n",
    "print(get_glove_recommendations('Avengers: Infinity War'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wK0IBZXWTPok"
   },
   "source": [
    "# Neural Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SSTQ-ooLWJVc",
    "outputId": "124aebe0-31e8-4be3-c1d8-c6f1347c9957"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 814
    },
    "id": "7bawWbcCX_uh",
    "outputId": "7ab4162e-e626-4228-ff97-9515993081a7"
   },
   "outputs": [],
   "source": [
    "# Encoding user_ids (for demonstration purposes, we simulate users)\n",
    "df['user_id'] = np.random.randint(0, 1000, size=len(df))  # Randomly simulate users\n",
    "\n",
    "# Label encoding for user and movie IDs\n",
    "user_encoder = LabelEncoder()\n",
    "df['user_id_encoded'] = user_encoder.fit_transform(df['user_id'])\n",
    "\n",
    "movie_encoder = LabelEncoder()\n",
    "df['movie_id_encoded'] = movie_encoder.fit_transform(df['show_id'])\n",
    "\n",
    "# Metadata preparation (e.g., genre, director, actors)\n",
    "# This will be used for content-based embedding extensions later on\n",
    "df['genre'] = df['listed_in'].fillna('')\n",
    "df['director'] = df['director'].fillna('')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train, test = train_test_split(df[['user_id_encoded', 'movie_id_encoded']], test_size=0.2)\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_size = 50  # Size of the embedding vectors for users and movies\n",
    "num_users = len(df['user_id_encoded'].unique())\n",
    "num_movies = len(df['movie_id_encoded'].unique())\n",
    "\n",
    "# Define input layers for users and movies\n",
    "user_input = Input(shape=(1,), name='user_input')\n",
    "movie_input = Input(shape=(1,), name='movie_input')\n",
    "\n",
    "# Embedding layers for users and movies\n",
    "user_embedding = Embedding(input_dim=num_users, output_dim=embedding_size, name='user_embedding')(user_input)\n",
    "movie_embedding = Embedding(input_dim=num_movies, output_dim=embedding_size, name='movie_embedding')(movie_input)\n",
    "\n",
    "# Flatten the embeddings to feed into the neural network\n",
    "user_vec = Flatten()(user_embedding)\n",
    "movie_vec = Flatten()(movie_embedding)\n",
    "\n",
    "# Dot product of user and movie vectors (to model user-movie interactions)\n",
    "dot_product = Dot(axes=1, normalize=True)([user_vec, movie_vec])\n",
    "\n",
    "# Output layer (predicting interaction strength, like a rating)\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "\n",
    "# Define the model\n",
    "model = Model([user_input, movie_input], output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Prepare data for training\n",
    "train_user_data = train['user_id_encoded'].values\n",
    "train_movie_data = train['movie_id_encoded'].values\n",
    "train_labels = np.random.randint(0, 2, size=len(train))  # Simulated interaction data (0/1)\n",
    "\n",
    "# Fit the model\n",
    "model.fit([train_user_data, train_movie_data], train_labels, epochs=10, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# Now you can use the trained embeddings for recommendation\n",
    "# For example, you can compute similarity between movie embeddings to recommend similar movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "J2Km5lc2_4lp",
    "outputId": "91bb0bbe-7c88-4405-e9e0-8bbe83869544"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_users = df['user_id'].nunique()  # Number of unique users\n",
    "n_movies = df['show_id'].nunique()  # Number of unique movies\n",
    "embedding_size = 50  # Size of user and movie embeddings\n",
    "dropout_rate = 0.5  # Dropout rate for regularization\n",
    "\n",
    "# Input layers for users and movies\n",
    "user_input = Input(shape=(1,), name='user_input')\n",
    "movie_input = Input(shape=(1,), name='movie_input')\n",
    "\n",
    "# Embedding layers for users and movies\n",
    "user_embedding = Embedding(input_dim=n_users, output_dim=embedding_size, name='user_embedding')(user_input)\n",
    "movie_embedding = Embedding(input_dim=n_movies, output_dim=embedding_size, name='movie_embedding')(movie_input)\n",
    "\n",
    "# Flatten the embeddings\n",
    "user_vec = Flatten()(user_embedding)\n",
    "movie_vec = Flatten()(movie_embedding)\n",
    "\n",
    "# Concatenate user and movie embeddings\n",
    "concat = Concatenate()([user_vec, movie_vec])\n",
    "\n",
    "# Dense layers to capture higher-order interactions\n",
    "dense_1 = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(concat)\n",
    "dropout_1 = Dropout(dropout_rate)(dense_1)\n",
    "dense_2 = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(dropout_1)\n",
    "dropout_2 = Dropout(dropout_rate)(dense_2)\n",
    "\n",
    "# Output layer (interaction prediction)\n",
    "output = Dense(1, activation='linear', name='output')(dropout_2)\n",
    "\n",
    "# Define the model\n",
    "ncf_model = Model([user_input, movie_input], output)\n",
    "\n",
    "# Compile the model\n",
    "ncf_model.compile(optimizer=Adam(learning_rate=0.0005), loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "\n",
    "# Print model summary\n",
    "ncf_model.summary()\n",
    "\n",
    "# Assuming you have the following prepared data:\n",
    "# X_train_user, X_train_movie: arrays of user and movie IDs for training\n",
    "# y_train: the target values (e.g., ratings or interaction labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train, test = train_test_split(df[['user_id_encoded', 'movie_id_encoded', 'rating']], test_size=0.2)\n",
    "\n",
    "# Prepare data for training\n",
    "X_train_user = train['user_id_encoded'].values\n",
    "X_train_movie = train['movie_id_encoded'].values\n",
    "y_train = np.random.randint(0, 2, size=len(train)) # Generate random labels for training since you don't have numerical ratings\n",
    "\n",
    "# Prepare data for testing\n",
    "X_test_user = test['user_id_encoded'].values\n",
    "X_test_movie = test['movie_id_encoded'].values\n",
    "y_test = np.random.randint(0, 2, size=len(test)) # Generate random labels for testing since you don't have numerical ratings\n",
    "\n",
    "# Train the model\n",
    "ncf_model.fit([X_train_user, X_train_movie], y_train, batch_size=64, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5YpMvaYqCPms",
    "outputId": "c2afb0c2-45df-4273-d7a7-80d6db583ec2"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def recommend_similar_movies(movie_name, movie_embeddings, df, top_n=10):\n",
    "\n",
    "    # Create a mapping from encoded IDs to movie titles\n",
    "    movie_idx_to_title = dict(zip(df['movie_id_encoded'], df['title']))\n",
    "\n",
    "    # Check if the given movie name is in the dataset\n",
    "    if movie_name not in df['title'].values:\n",
    "        print(f\"Movie '{movie_name}' not found in the dataset.\")\n",
    "        return []\n",
    "\n",
    "    # Get the encoded ID for the given movie name\n",
    "    movie_idx = df[df['title'] == movie_name].iloc[0]['movie_id_encoded']\n",
    "\n",
    "    # Get the embedding for the given movie\n",
    "    target_movie_embedding = movie_embeddings[movie_idx].reshape(1, -1)\n",
    "\n",
    "    # Compute cosine similarity between the target movie embedding and all other movie embeddings\n",
    "    similarities = cosine_similarity(target_movie_embedding, movie_embeddings)[0]\n",
    "\n",
    "    # Create a dataframe to hold movie indices and similarity scores\n",
    "    similarity_df = pd.DataFrame({\n",
    "        'movie_id_encoded': np.arange(len(similarities)),\n",
    "        'similarity_score': similarities\n",
    "    })\n",
    "\n",
    "    # Exclude the target movie itself from the recommendations\n",
    "    similarity_df = similarity_df[similarity_df['movie_id_encoded'] != movie_idx]\n",
    "\n",
    "    # Get top N most similar movies\n",
    "    top_similar_movies = similarity_df.sort_values(by='similarity_score', ascending=False).head(top_n)\n",
    "\n",
    "    # Map encoded IDs back to movie titles\n",
    "    recommended_titles = top_similar_movies['movie_id_encoded'].map(lambda x: movie_idx_to_title.get(x, \"Unknown Title\")).values\n",
    "\n",
    "    return recommended_titles\n",
    "\n",
    "# Assuming you have a trained movie embedding layer\n",
    "# We extract the weights from the embedding layer to get movie embeddings\n",
    "movie_embeddings_weights = ncf_model.get_layer('movie_embedding').get_weights()[0]\n",
    "\n",
    "# Example usage:\n",
    "movie_name = \"Avengers: Infinity War\"  # Replace with any movie name to get similar movies\n",
    "similar_movies = recommend_similar_movies(movie_name, movie_embeddings_weights, df, top_n=10)\n",
    "\n",
    "print(f\"Top 10 movies similar to '{movie_name}':\")\n",
    "for idx, title in enumerate(similar_movies, 1):\n",
    "    print(f\"{idx}. {title}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
